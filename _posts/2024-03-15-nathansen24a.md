---
abstract: Designing artificial proteins with specialized functions promises new solutions
  for biological, medical, and environmental use cases. This field benefits from advances
  in natural language processing, with state-of-the-art text generation models already
  being successfully applied to protein sequences. Openly available pre-trained protein
  language models are able to generate artificial protein sequences and can be finetuned
  on very specific tasks. Considering the high computational cost of finetuning a
  model exclusively for one downstream task, prompt tuning has been proposed as a
  more cost-efficient alternative that shares one model across different tasks. However,
  no openly available implementation of this approach compatible with protein language
  models has been previously published. Thus, we adapt an open-source codebase designed
  for NLP models to build a pipeline for prompt tuning on protein sequence data, supporting
  the protein language models ProtGPT2 and RITA.  We benchmark this implementation
  for generating proteins of a specific family and evaluate the approach using text
  processing metrics as well as family membership prediction and protein activity
  prediction of generated sequences. Our results confirm the advantages of prompt
  tuning in resource usage, especially storage, encouraging further research and expansion
  of this technique to related use cases. For our evaluated use case, prompt tuning
  does not reach up to finetuning in terms of the quality of generated protein sequences,
  indicating the need for more extensive optimization. Lastly, we observe discrepancies
  between results of similar evaluation tools, highlighting open problems for principled
  assessment of protein sequence generation quality.
title: Evaluating Tuning Strategies for Sequence Generation with Protein Language
  Models
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nathansen24a
month: 0
tex_title: Evaluating Tuning Strategies for Sequence Generation with Protein Language
  Models
firstpage: 76
lastpage: 89
page: 76-89
order: 76
cycles: false
bibtex_author: Nathansen, Andrea and Klein, Kevin and Renard, Bernhard and Nowicka,
  Melania and Bartoszewicz, Jakub M
author:
- given: Andrea
  family: Nathansen
- given: Kevin
  family: Klein
- given: Bernhard
  family: Renard
- given: Melania
  family: Nowicka
- given: Jakub M
  family: Bartoszewicz
date: 2024-03-15
address:
container-title: Proceedings of the 18th Machine Learning in Computational Biology
  meeting
volume: '240'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v240/nathansen24a/nathansen24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
